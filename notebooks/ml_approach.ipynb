{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 4.3.3\n"
     ]
    }
   ],
   "source": [
    "print(\"version:\", gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download pretrained model (~1.6GB)\n",
    "# word2vec = api.load(\"word2vec-google-news-300\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get current working directory (where notebook is run)\n",
    "PROJECT_ROOT = Path().absolute()  # or Path.cwd()\n",
    "TRAIN_PATH = PROJECT_ROOT / os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"Flicker8k_Dataset\", \"train\")\n",
    "TEST_PATH = PROJECT_ROOT / os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"Flicker8k_Dataset\", \"test\")\n",
    "VAL_PATH = PROJECT_ROOT / os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"Flicker8k_Dataset\", \"val\")\n",
    "descriptions_path = PROJECT_ROOT / os.path.join(PROJECT_ROOT, \"data\", \"raw\", \"Flicker8k_Dataset\", \"Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from images and caption words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_image_features(image):\n",
    "#     \"\"\"Color histograms + edge features\"\"\"\n",
    "#     hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "#     edges = cv2.Canny(image, 100, 200)\n",
    "#     return np.concatenate([hist.flatten(), edges.flatten()])\n",
    "\n",
    "# def word_features(prev_word, image_feats):\n",
    "#     \"\"\"Combine linguistic and visual features\"\"\"\n",
    "#     return np.hstack([\n",
    "#         image_feats,\n",
    "#         word2vec[prev_word] if prev_word in word2vec else np.zeros(100)\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, val_dir = '/Users/ruhwang/Desktop/AI/spring2025_courses/aipi540-dl/caption_generator/data/raw/Flicker8k_Dataset/train', '/Users/ruhwang/Desktop/AI/spring2025_courses/aipi540-dl/caption_generator/data/raw/Flicker8k_Dataset/val'\n",
    "test_dir = '/Users/ruhwang/Desktop/AI/spring2025_courses/aipi540-dl/caption_generator/data/raw/Flicker8k_Dataset/test'\n",
    "tokens_dir = '/Users/ruhwang/Desktop/AI/spring2025_courses/aipi540-dl/caption_generator/data/raw/Flicker8k_Dataset/Flickr8k.token.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train MEMM (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from gensim.models import KeyedVectors\n",
    "import joblib  # For caching\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components with memoization\n",
    "class FeatureProcessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=0.95)\n",
    "        self._is_fitted = False\n",
    "        self.cache_path = \"./feature_cache\"\n",
    "        os.makedirs(self.cache_path, exist_ok=True)\n",
    "        \n",
    "    def normalize(self, features):\n",
    "        if not self._is_fitted:\n",
    "            features = self.scaler.fit_transform(features)\n",
    "            features = self.pca.fit_transform(features)\n",
    "            self._is_fitted = True\n",
    "        else:\n",
    "            features = self.scaler.transform(features)\n",
    "            features = self.pca.transform(features)\n",
    "        return features\n",
    "\n",
    "# Initialize at global scope\n",
    "fp = FeatureProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMMCaptionGenerator:\n",
    "    def __init__(self, cache_path=\"./feature_cache\"):\n",
    "        self.model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            C=0.1,\n",
    "            penalty='l2',\n",
    "            solver='saga',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        self.vec = DictVectorizer(sparse=False)\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word2vec = None  # Initialize as None, will be loaded later\n",
    "        self.cache_path = cache_path\n",
    "        os.makedirs(self.cache_path, exist_ok=True)\n",
    "        \n",
    "        # Initialize feature processor\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.decomposition import PCA\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=0.95)\n",
    "\n",
    "    def train(self, train_images, train_captions, val_images=None, val_captions=None, patience=3):\n",
    "        \"\"\"Train the MEMM model with optional early stopping\"\"\"\n",
    "        # Load word embeddings if not already loaded\n",
    "        if self.word2vec is None:\n",
    "            self.load_word_embeddings()\n",
    "        \n",
    "        # Create vocabulary\n",
    "        all_words = [word for caps in train_captions for cap in caps for word in cap]\n",
    "        self.word2idx = {w: i for i, w in enumerate(set(all_words))}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y = [], []\n",
    "        for img_path, caps in zip(train_images, train_captions):\n",
    "            img_feats = self.extract_image_features(img_path)\n",
    "            for cap in caps:\n",
    "                for i in range(1, len(cap)):\n",
    "                    features = self.word_features(img_path, cap[i-1], img_feats)\n",
    "                    feat_dict = self.create_feature_dict(cap[i-1], img_feats, features)\n",
    "                    X.append(feat_dict)\n",
    "                    y.append(self.word2idx[cap[i]])\n",
    "        \n",
    "        # Vectorize and train\n",
    "        X_vec = self.vec.fit_transform(X)\n",
    "        \n",
    "        if val_images:\n",
    "            return self.train_with_early_stopping(X_vec, y, val_images, val_captions, patience)\n",
    "        else:\n",
    "            self.model.fit(X_vec, y)\n",
    "            return self\n",
    "\n",
    "    def train_with_early_stopping(self, X_vec, y, val_images, val_captions, patience):\n",
    "        \"\"\"Helper method for early stopping during training\"\"\"\n",
    "        best_bleu = -1\n",
    "        no_improve = 0\n",
    "        best_weights = None\n",
    "        \n",
    "        for epoch in range(100):  # Max epochs\n",
    "            self.model.fit(X_vec, y)\n",
    "            \n",
    "            # Validate\n",
    "            val_bleu = self.evaluate(val_images, val_captions)\n",
    "            print(f\"Epoch {epoch+1}: Val BLEU = {val_bleu:.4f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_bleu > best_bleu:\n",
    "                best_bleu = val_bleu\n",
    "                no_improve = 0\n",
    "                best_weights = self.model.coef_.copy()\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    self.model.coef_ = best_weights\n",
    "                    break\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def evaluate(self, val_images, val_captions):\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        \n",
    "        for img_path, caps in zip(val_images, val_captions):\n",
    "            gen_caption = self.generate_caption(img_path)\n",
    "            hypotheses.append(gen_caption.split())\n",
    "            references.append(caps)\n",
    "        \n",
    "        from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "        smooth = SmoothingFunction().method1\n",
    "        return corpus_bleu(references, hypotheses, smoothing_function=smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (75.8.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.7.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.1\n",
      "  Downloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.3.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.3.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.3.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.3.1) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.3.1) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "Downloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.0\n",
      "    Uninstalling torch-2.7.0:\n",
      "      Successfully uninstalled torch-2.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.22.0 requires torch==2.7.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torchvision==0.17.2\n",
      "  Downloading torchvision-0.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torchvision==0.17.2) (1.26.4)\n",
      "Collecting torch==2.2.2 (from torchvision==0.17.2)\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torchvision==0.17.2) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.2.2->torchvision==0.17.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.2.2->torchvision==0.17.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.2.2->torchvision==0.17.2) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from torch==2.2.2->torchvision==0.17.2) (2025.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from jinja2->torch==2.2.2->torchvision==0.17.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ruhwang/Desktop/AI/spring2025_courses/.venv/lib/python3.12/site-packages (from sympy->torch==2.2.2->torchvision==0.17.2) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp312-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.0\n",
      "    Uninstalling torchvision-0.22.0:\n",
      "      Successfully uninstalled torchvision-0.22.0\n",
      "Successfully installed torch-2.2.2 torchvision-0.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch==2.3.1\n",
    "!pip install torchvision==0.17.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "# import torchvision.models as models\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # Garbage collection\n",
    "import time\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "\n",
    "class MEMMCaptionGenerator:\n",
    "    def __init__(self, glove_path=None, cache_path=\"./feature_cache\", device='cpu',\n",
    "                 batch_size=32, use_pca=True, pca_components=0.95, embedding_dim=100):\n",
    "        self.model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            C=0.1,\n",
    "            penalty='l2',\n",
    "            solver='saga',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        self.vec = DictVectorizer(sparse=True)\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.glove_path = glove_path\n",
    "        self.word_embeddings = {}\n",
    "        self.embedding_dim = embedding_dim  # Default embedding dimension\n",
    "        self.cache_path = cache_path\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.use_pca = use_pca\n",
    "        os.makedirs(self.cache_path, exist_ok=True)\n",
    "\n",
    "        # Feature processing\n",
    "        self.scaler = StandardScaler()\n",
    "        if use_pca:\n",
    "            self.pca = PCA(n_components=pca_components)\n",
    "        self.is_fitted = False\n",
    "\n",
    "        # CNN feature extractor - load only when needed\n",
    "        self.cnn_model = None\n",
    "        self.transform = None\n",
    "\n",
    "    def _load_cnn_model(self):\n",
    "        \"\"\"Lazy loading of CNN model to save memory\"\"\"\n",
    "        if self.cnn_model is None:\n",
    "            print(\"Loading CNN model...\")\n",
    "            self.cnn_model = mobilenet_v2(pretrained=True)\n",
    "            self.cnn_model = torch.nn.Sequential(*list(self.cnn_model.children())[:-1])\n",
    "            self.cnn_model.eval()\n",
    "            self.cnn_model.to(self.device)\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def download_glove_embeddings(self, dimension=100):\n",
    "        \"\"\"Download GloVe embeddings if not available\"\"\"\n",
    "        glove_dir = \"./glove\"\n",
    "        os.makedirs(glove_dir, exist_ok=True)\n",
    "\n",
    "        # Set file paths\n",
    "        zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "        glove_path = os.path.join(glove_dir, f\"glove.6B.{dimension}d.txt\")\n",
    "\n",
    "        # Check if file already exists\n",
    "        if os.path.exists(glove_path):\n",
    "            print(f\"GloVe embeddings already exist at {glove_path}\")\n",
    "            self.glove_path = glove_path\n",
    "            return glove_path\n",
    "\n",
    "        # Download if needed\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(\"Downloading GloVe embeddings...\")\n",
    "            url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, zip_path)\n",
    "                print(f\"Downloaded to {zip_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading GloVe embeddings: {e}\")\n",
    "                print(\"Please download manually from https://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "                print(\"and extract the files to ./glove/ directory\")\n",
    "                return None\n",
    "\n",
    "        # Extract if needed\n",
    "        if not os.path.exists(glove_path):\n",
    "            print(f\"Extracting {dimension}d GloVe embeddings...\")\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extract(f\"glove.6B.{dimension}d.txt\", glove_dir)\n",
    "                print(f\"Extracted to {glove_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting GloVe embeddings: {e}\")\n",
    "                return None\n",
    "\n",
    "        self.glove_path = glove_path\n",
    "        return glove_path\n",
    "\n",
    "    def load_glove_embeddings(self):\n",
    "        \"\"\"Load GloVe embeddings directly from file\"\"\"\n",
    "        if not self.word_embeddings:\n",
    "            # Try to download if path not provided or file doesn't exist\n",
    "            if not self.glove_path or not os.path.exists(self.glove_path):\n",
    "                self.glove_path = self.download_glove_embeddings(dimension=self.embedding_dim)\n",
    "\n",
    "            if not self.glove_path or not os.path.exists(self.glove_path):\n",
    "                print(\"WARNING: No GloVe embeddings available. Using random embeddings instead.\")\n",
    "                return\n",
    "\n",
    "            print(f\"Loading GloVe embeddings from {self.glove_path}...\")\n",
    "            self.word_embeddings = {}\n",
    "            try:\n",
    "                with open(self.glove_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        values = line.strip().split()\n",
    "                        word = values[0]\n",
    "                        vector = np.array(values[1:], dtype='float32')\n",
    "                        self.word_embeddings[word] = vector\n",
    "\n",
    "                        # Set embedding dimension based on first vector\n",
    "                        if self.embedding_dim is None:\n",
    "                            self.embedding_dim = len(vector)\n",
    "\n",
    "                print(f\"Loaded {len(self.word_embeddings)} word vectors with dimension {self.embedding_dim}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading GloVe embeddings: {e}\")\n",
    "                print(\"Using random embeddings instead.\")\n",
    "        else:\n",
    "            print(\"Embeddings already loaded\")\n",
    "\n",
    "    def get_feature_cache_key(self, img_path, prev_word):\n",
    "        \"\"\"Generate a unique cache key for features\"\"\"\n",
    "        img_name = os.path.basename(img_path)\n",
    "        return f\"{img_name}_{prev_word}.pkl\"\n",
    "\n",
    "    def extract_image_features(self, image_path):\n",
    "        \"\"\"Extract image features with caching\"\"\"\n",
    "        # Check if features are already cached\n",
    "        cache_file = os.path.join(self.cache_path, f\"{os.path.basename(image_path)}_features.pkl\")\n",
    "        if os.path.exists(cache_file):\n",
    "            return joblib.load(cache_file)\n",
    "\n",
    "        # Load CNN model if not already loaded\n",
    "        self._load_cnn_model()\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # CNN features\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            cnn_features = self.cnn_model(img_tensor).squeeze().cpu().numpy().flatten()\n",
    "\n",
    "        # Traditional features - simplified to reduce computation\n",
    "        img_cv = cv2.imread(image_path)\n",
    "        if img_cv is None:\n",
    "            raise ValueError(f\"Could not load image at {image_path}\")\n",
    "\n",
    "        # Simplified color histogram (reduced bins)\n",
    "        hsv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2HSV)\n",
    "        hist_hsv = cv2.calcHist([hsv], [0, 1], None, [8, 8], [0, 180, 0, 256]).flatten()\n",
    "\n",
    "        # Simplified edge features\n",
    "        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        edge_hist = cv2.calcHist([edges], [0], None, [16], [0, 256]).flatten()\n",
    "\n",
    "        features = {\n",
    "            'cnn_features': cnn_features,\n",
    "            'color_hist': hist_hsv,\n",
    "            'edge_feats': edge_hist\n",
    "        }\n",
    "\n",
    "        # Cache the features\n",
    "        joblib.dump(features, cache_file)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def create_feature_dict(self, prev_word, img_feats, word_embed):\n",
    "        \"\"\"Create feature dictionary for vectorization - optimized version\"\"\"\n",
    "        # Create a dictionary with prev_word as a categorical feature\n",
    "        feature_dict = {'prev_word': prev_word}\n",
    "\n",
    "        # Add numerical features with proper naming - use fewer features\n",
    "        # For CNN features - use only a subset to reduce dimensionality\n",
    "        cnn_step = max(1, len(img_feats['cnn_features']) // 100)  # Sample only ~100 CNN features\n",
    "        for i, val in enumerate(img_feats['cnn_features'][::cnn_step]):\n",
    "            feature_dict[f'cnn_{i}'] = float(val)\n",
    "\n",
    "        # For color histogram - use all\n",
    "        for i, val in enumerate(img_feats['color_hist']):\n",
    "            feature_dict[f'color_{i}'] = float(val)\n",
    "\n",
    "        # For edge features - use all\n",
    "        for i, val in enumerate(img_feats['edge_feats']):\n",
    "            feature_dict[f'edge_{i}'] = float(val)\n",
    "\n",
    "        # For word embeddings - use only a subset to reduce dimensionality\n",
    "        embed_step = max(1, len(word_embed) // 50)  # Sample only ~50 embedding features\n",
    "        for i, val in enumerate(word_embed[::embed_step]):\n",
    "            feature_dict[f'embed_{i}'] = float(val)\n",
    "\n",
    "        return feature_dict\n",
    "\n",
    "    def get_word_embedding(self, word):\n",
    "        \"\"\"Get word embedding with fallback to zero vector\"\"\"\n",
    "        if not self.word_embeddings:\n",
    "            self.load_glove_embeddings()\n",
    "\n",
    "        # If still no embeddings or word not found, use random embedding\n",
    "        if not self.word_embeddings or word not in self.word_embeddings:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)\n",
    "\n",
    "        return self.word_embeddings[word]\n",
    "\n",
    "    def train(self, train_images, train_captions, val_images=None, val_captions=None, patience=3):\n",
    "        \"\"\"Train the MEMM model with memory-efficient batching\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create vocabulary\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_words = set()\n",
    "        for caps in train_captions:\n",
    "            for cap in caps:\n",
    "                all_words.update(cap)\n",
    "\n",
    "        self.word2idx = {w: i for i, w in enumerate(all_words)}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "\n",
    "        # Process in batches to save memory\n",
    "        X_batches = []\n",
    "        y_batches = []\n",
    "\n",
    "        total_images = len(train_images)\n",
    "        for batch_start in range(0, total_images, self.batch_size):\n",
    "            batch_end = min(batch_start + self.batch_size, total_images)\n",
    "            print(f\"Processing batch {batch_start//self.batch_size + 1}/{(total_images-1)//self.batch_size + 1} \"\n",
    "                  f\"(images {batch_start+1}-{batch_end}/{total_images})\")\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            # Process each image in the batch\n",
    "            for img_idx in range(batch_start, batch_end):\n",
    "                img_path = train_images[img_idx]\n",
    "                caps = train_captions[img_idx]\n",
    "\n",
    "                # Extract image features (cached)\n",
    "                try:\n",
    "                    img_feats = self.extract_image_features(img_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Process each caption\n",
    "                for cap in caps:\n",
    "                    for i in range(1, len(cap)):\n",
    "                        prev_word = cap[i-1]\n",
    "                        target_word = cap[i]\n",
    "\n",
    "                        # Get word embedding\n",
    "                        word_embed = self.get_word_embedding(prev_word)\n",
    "\n",
    "                        # Create feature dictionary\n",
    "                        feat_dict = self.create_feature_dict(prev_word, img_feats, word_embed)\n",
    "\n",
    "                        X_batch.append(feat_dict)\n",
    "                        y_batch.append(self.word2idx[target_word])\n",
    "\n",
    "            # Vectorize this batch\n",
    "            if X_batch:  # Only process if batch is not empty\n",
    "                X_vec_batch = self.vec.fit_transform(X_batch) if not X_batches else self.vec.transform(X_batch)\n",
    "                X_batches.append(X_vec_batch)\n",
    "                y_batches.extend(y_batch)\n",
    "\n",
    "                # Clear memory\n",
    "                del X_batch, y_batch\n",
    "                gc.collect()\n",
    "\n",
    "        # Combine all batches\n",
    "        print(\"Combining batches...\")\n",
    "        from scipy.sparse import vstack\n",
    "        X_vec = vstack(X_batches) if all(hasattr(x, 'shape') for x in X_batches) else None\n",
    "        y = np.array(y_batches)\n",
    "\n",
    "        # Clear memory\n",
    "        del X_batches, y_batches\n",
    "        gc.collect()\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Training model on {X_vec.shape[0]} examples with {X_vec.shape[1]} features...\")\n",
    "        self.model.fit(X_vec, y)\n",
    "\n",
    "        print(f\"Training completed in {(time.time() - start_time)/60:.2f} minutes\")\n",
    "        return self\n",
    "\n",
    "    def generate_caption(self, image_path, beam_width=3, max_length=20):\n",
    "        \"\"\"Generate caption using beam search - simplified version\"\"\"\n",
    "        # Extract image features\n",
    "        img_feats = self.extract_image_features(image_path)\n",
    "\n",
    "        # Initialize beam\n",
    "        beam = [{'sequence': ['<start>'], 'score': 0.0}]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            if all(s['sequence'][-1] == '<end>' for s in beam):\n",
    "                break\n",
    "\n",
    "            new_beam = []\n",
    "\n",
    "            for state in beam:\n",
    "                if state['sequence'][-1] == '<end>':\n",
    "                    new_beam.append(state)\n",
    "                    continue\n",
    "\n",
    "                # Get word embedding for previous word\n",
    "                prev_word = state['sequence'][-1]\n",
    "                word_embed = self.get_word_embedding(prev_word)\n",
    "\n",
    "                # Create feature dictionary\n",
    "                feat_dict = self.create_feature_dict(prev_word, img_feats, word_embed)\n",
    "\n",
    "                # Predict next words\n",
    "                feats = self.vec.transform([feat_dict])\n",
    "                log_probs = self.model.predict_log_proba(feats)[0]\n",
    "\n",
    "                # Get top candidates\n",
    "                top_indices = np.argsort(log_probs)[-beam_width:]\n",
    "                for idx in top_indices:\n",
    "                    word = self.idx2word[idx]\n",
    "                    new_score = state['score'] + log_probs[idx]\n",
    "                    new_sequence = state['sequence'] + [word]\n",
    "                    new_beam.append({\n",
    "                        'sequence': new_sequence,\n",
    "                        'score': new_score\n",
    "                    })\n",
    "\n",
    "            # Select top beam_width candidates\n",
    "            beam = sorted(new_beam, key=lambda x: x['score'], reverse=True)[:beam_width]\n",
    "\n",
    "        # Return best sequence\n",
    "        best_sequence = max(beam, key=lambda x: x['score'])['sequence']\n",
    "        return ' '.join(best_sequence[1:-1])  # Remove <start> and <end>\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save model to disk\"\"\"\n",
    "        print(f\"Saving model to {path}...\")\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'vec': self.vec,\n",
    "            'word2idx': self.word2idx,\n",
    "            'idx2word': self.idx2word,\n",
    "            'scaler': self.scaler,\n",
    "            'is_fitted': self.is_fitted,\n",
    "            'embedding_dim': self.embedding_dim\n",
    "        }\n",
    "        if self.use_pca:\n",
    "            model_data['pca'] = self.pca\n",
    "\n",
    "        joblib.dump(model_data, path)\n",
    "        print(\"Model saved successfully\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load model from disk\"\"\"\n",
    "        print(f\"Loading model from {path}...\")\n",
    "        model_data = joblib.load(path)\n",
    "        self.model = model_data['model']\n",
    "        self.vec = model_data['vec']\n",
    "        self.word2idx = model_data['word2idx']\n",
    "        self.idx2word = model_data['idx2word']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.is_fitted = model_data['is_fitted']\n",
    "        self.embedding_dim = model_data.get('embedding_dim', 100)\n",
    "        if 'pca' in model_data:\n",
    "            self.pca = model_data['pca']\n",
    "            self.use_pca = True\n",
    "        print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image_captions(descriptions_path):\n",
    "#     \"\"\"Load {image_id.jpg: [captions]} dictionary\"\"\"\n",
    "#     desc_dict = defaultdict(list)\n",
    "#     with open(descriptions_path, 'r') as f:\n",
    "#         for line in f:\n",
    "#             img_file, caption = line.strip().split('\\t', 1)\n",
    "#             desc_dict[img_file].append(caption.split())\n",
    "#     return desc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dir = '/Users/ruhwang/Desktop/AI/spring2025_courses/aipi540-dl/caption_generator/data/raw/Flickr8k_text/Flickr8k.token.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def prepare_data(image_dir, desc_dict):\n",
    "    \"\"\"Return (images, captions) pairs for a directory\"\"\"\n",
    "    images, captions = [], []\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if img_file.endswith('.jpg') and img_file in desc_dict:\n",
    "            images.append(os.path.join(image_dir, img_file))\n",
    "            captions.append(desc_dict[img_file])  # All reference captions\n",
    "    return images, captions\n",
    "\n",
    "def load_image_captions(tokens_file):\n",
    "    \"\"\"Load {image_filename: [list of tokenized captions]} from tokens file\"\"\"\n",
    "    captions = defaultdict(list)\n",
    "    with open(tokens_file) as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:  # Ensure proper format\n",
    "                    img_file = parts[0].split('#')[0]  # Remove #0, #1 etc.\n",
    "                    caption = parts[1].lower().split()  # Tokenized and lowercase\n",
    "                    captions[img_file].append(caption)\n",
    "    return dict(captions)\n",
    "\n",
    "def get_image_paths(image_dir, captions_dict):\n",
    "    \"\"\"Get aligned (image_path, captions) pairs for a directory\"\"\"\n",
    "    return [\n",
    "        (os.path.join(image_dir, img_file), captions_dict[img_file])\n",
    "        for img_file in os.listdir(image_dir)\n",
    "        if img_file.endswith('.jpg') and img_file in captions_dict\n",
    "    ]\n",
    "\n",
    "# Load all captions from token file\n",
    "all_captions = load_image_captions(tokens_dir)\n",
    "\n",
    "# Organize into splits\n",
    "train_pairs = get_image_paths(train_dir, all_captions)\n",
    "val_pairs = get_image_paths(val_dir, all_captions)\n",
    "test_pairs = get_image_paths(test_dir, all_captions)\n",
    "\n",
    "# Separate into images and captions\n",
    "train_images, train_captions = zip(*train_pairs) if train_pairs else ([], [])\n",
    "val_images, val_captions = zip(*val_pairs) if val_pairs else ([], [])\n",
    "test_images, test_captions = zip(*test_pairs) if test_pairs else ([], [])\n",
    "\n",
    "# Convert tuples to lists (for easier modification)\n",
    "train_images, train_captions = list(train_images), list(train_captions)\n",
    "val_images, val_captions = list(val_images), list(val_captions)\n",
    "test_images, test_captions = list(test_images), list(test_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Vocabulary size: 7600\n",
      "Processing batch 1/354 (images 1-16/5663)\n",
      "Downloading GloVe embeddings...\n",
      "Downloaded to ./glove/glove.6B.zip\n",
      "Extracting 100d GloVe embeddings...\n",
      "Extracted to ./glove/glove.6B.100d.txt\n",
      "Loading GloVe embeddings from ./glove/glove.6B.100d.txt...\n",
      "Loaded 400000 word vectors with dimension 100\n",
      "Loading CNN model...\n",
      "Processing batch 2/354 (images 17-32/5663)\n",
      "Processing batch 3/354 (images 33-48/5663)\n",
      "Processing batch 4/354 (images 49-64/5663)\n",
      "Processing batch 5/354 (images 65-80/5663)\n",
      "Processing batch 6/354 (images 81-96/5663)\n",
      "Processing batch 7/354 (images 97-112/5663)\n",
      "Processing batch 8/354 (images 113-128/5663)\n",
      "Processing batch 9/354 (images 129-144/5663)\n",
      "Processing batch 10/354 (images 145-160/5663)\n",
      "Processing batch 11/354 (images 161-176/5663)\n",
      "Processing batch 12/354 (images 177-192/5663)\n",
      "Processing batch 13/354 (images 193-208/5663)\n",
      "Processing batch 14/354 (images 209-224/5663)\n",
      "Processing batch 15/354 (images 225-240/5663)\n",
      "Processing batch 16/354 (images 241-256/5663)\n",
      "Processing batch 17/354 (images 257-272/5663)\n",
      "Processing batch 18/354 (images 273-288/5663)\n",
      "Processing batch 19/354 (images 289-304/5663)\n",
      "Processing batch 20/354 (images 305-320/5663)\n",
      "Processing batch 21/354 (images 321-336/5663)\n",
      "Processing batch 22/354 (images 337-352/5663)\n",
      "Processing batch 23/354 (images 353-368/5663)\n",
      "Processing batch 24/354 (images 369-384/5663)\n",
      "Processing batch 25/354 (images 385-400/5663)\n",
      "Processing batch 26/354 (images 401-416/5663)\n",
      "Processing batch 27/354 (images 417-432/5663)\n",
      "Processing batch 28/354 (images 433-448/5663)\n",
      "Processing batch 29/354 (images 449-464/5663)\n",
      "Processing batch 30/354 (images 465-480/5663)\n",
      "Processing batch 31/354 (images 481-496/5663)\n",
      "Processing batch 32/354 (images 497-512/5663)\n",
      "Processing batch 33/354 (images 513-528/5663)\n",
      "Processing batch 34/354 (images 529-544/5663)\n",
      "Processing batch 35/354 (images 545-560/5663)\n",
      "Processing batch 36/354 (images 561-576/5663)\n",
      "Processing batch 37/354 (images 577-592/5663)\n",
      "Processing batch 38/354 (images 593-608/5663)\n",
      "Processing batch 39/354 (images 609-624/5663)\n",
      "Processing batch 40/354 (images 625-640/5663)\n",
      "Processing batch 41/354 (images 641-656/5663)\n",
      "Processing batch 42/354 (images 657-672/5663)\n",
      "Processing batch 43/354 (images 673-688/5663)\n",
      "Processing batch 44/354 (images 689-704/5663)\n",
      "Processing batch 45/354 (images 705-720/5663)\n",
      "Processing batch 46/354 (images 721-736/5663)\n",
      "Processing batch 47/354 (images 737-752/5663)\n",
      "Processing batch 48/354 (images 753-768/5663)\n",
      "Processing batch 49/354 (images 769-784/5663)\n",
      "Processing batch 50/354 (images 785-800/5663)\n",
      "Processing batch 51/354 (images 801-816/5663)\n",
      "Processing batch 52/354 (images 817-832/5663)\n",
      "Processing batch 53/354 (images 833-848/5663)\n",
      "Processing batch 54/354 (images 849-864/5663)\n",
      "Processing batch 55/354 (images 865-880/5663)\n",
      "Processing batch 56/354 (images 881-896/5663)\n",
      "Processing batch 57/354 (images 897-912/5663)\n",
      "Processing batch 58/354 (images 913-928/5663)\n",
      "Processing batch 59/354 (images 929-944/5663)\n",
      "Processing batch 60/354 (images 945-960/5663)\n",
      "Processing batch 61/354 (images 961-976/5663)\n",
      "Processing batch 62/354 (images 977-992/5663)\n",
      "Processing batch 63/354 (images 993-1008/5663)\n",
      "Processing batch 64/354 (images 1009-1024/5663)\n",
      "Processing batch 65/354 (images 1025-1040/5663)\n",
      "Processing batch 66/354 (images 1041-1056/5663)\n",
      "Processing batch 67/354 (images 1057-1072/5663)\n",
      "Processing batch 68/354 (images 1073-1088/5663)\n",
      "Processing batch 69/354 (images 1089-1104/5663)\n",
      "Processing batch 70/354 (images 1105-1120/5663)\n",
      "Processing batch 71/354 (images 1121-1136/5663)\n",
      "Processing batch 72/354 (images 1137-1152/5663)\n",
      "Processing batch 73/354 (images 1153-1168/5663)\n",
      "Processing batch 74/354 (images 1169-1184/5663)\n",
      "Processing batch 75/354 (images 1185-1200/5663)\n",
      "Processing batch 76/354 (images 1201-1216/5663)\n",
      "Processing batch 77/354 (images 1217-1232/5663)\n",
      "Processing batch 78/354 (images 1233-1248/5663)\n",
      "Processing batch 79/354 (images 1249-1264/5663)\n",
      "Processing batch 80/354 (images 1265-1280/5663)\n",
      "Processing batch 81/354 (images 1281-1296/5663)\n",
      "Processing batch 82/354 (images 1297-1312/5663)\n",
      "Processing batch 83/354 (images 1313-1328/5663)\n",
      "Processing batch 84/354 (images 1329-1344/5663)\n",
      "Processing batch 85/354 (images 1345-1360/5663)\n",
      "Processing batch 86/354 (images 1361-1376/5663)\n",
      "Processing batch 87/354 (images 1377-1392/5663)\n",
      "Processing batch 88/354 (images 1393-1408/5663)\n",
      "Processing batch 89/354 (images 1409-1424/5663)\n",
      "Processing batch 90/354 (images 1425-1440/5663)\n",
      "Processing batch 91/354 (images 1441-1456/5663)\n",
      "Processing batch 92/354 (images 1457-1472/5663)\n",
      "Processing batch 93/354 (images 1473-1488/5663)\n",
      "Processing batch 94/354 (images 1489-1504/5663)\n",
      "Processing batch 95/354 (images 1505-1520/5663)\n",
      "Processing batch 96/354 (images 1521-1536/5663)\n",
      "Processing batch 97/354 (images 1537-1552/5663)\n",
      "Processing batch 98/354 (images 1553-1568/5663)\n",
      "Processing batch 99/354 (images 1569-1584/5663)\n",
      "Processing batch 100/354 (images 1585-1600/5663)\n",
      "Processing batch 101/354 (images 1601-1616/5663)\n",
      "Processing batch 102/354 (images 1617-1632/5663)\n",
      "Processing batch 103/354 (images 1633-1648/5663)\n",
      "Processing batch 104/354 (images 1649-1664/5663)\n",
      "Processing batch 105/354 (images 1665-1680/5663)\n",
      "Processing batch 106/354 (images 1681-1696/5663)\n",
      "Processing batch 107/354 (images 1697-1712/5663)\n",
      "Processing batch 108/354 (images 1713-1728/5663)\n",
      "Processing batch 109/354 (images 1729-1744/5663)\n",
      "Processing batch 110/354 (images 1745-1760/5663)\n",
      "Processing batch 111/354 (images 1761-1776/5663)\n",
      "Processing batch 112/354 (images 1777-1792/5663)\n",
      "Processing batch 113/354 (images 1793-1808/5663)\n",
      "Processing batch 114/354 (images 1809-1824/5663)\n",
      "Processing batch 115/354 (images 1825-1840/5663)\n",
      "Processing batch 116/354 (images 1841-1856/5663)\n",
      "Processing batch 117/354 (images 1857-1872/5663)\n",
      "Processing batch 118/354 (images 1873-1888/5663)\n",
      "Processing batch 119/354 (images 1889-1904/5663)\n",
      "Processing batch 120/354 (images 1905-1920/5663)\n",
      "Processing batch 121/354 (images 1921-1936/5663)\n",
      "Processing batch 122/354 (images 1937-1952/5663)\n",
      "Processing batch 123/354 (images 1953-1968/5663)\n",
      "Processing batch 124/354 (images 1969-1984/5663)\n",
      "Processing batch 125/354 (images 1985-2000/5663)\n",
      "Processing batch 126/354 (images 2001-2016/5663)\n",
      "Processing batch 127/354 (images 2017-2032/5663)\n",
      "Processing batch 128/354 (images 2033-2048/5663)\n",
      "Processing batch 129/354 (images 2049-2064/5663)\n",
      "Processing batch 130/354 (images 2065-2080/5663)\n",
      "Processing batch 131/354 (images 2081-2096/5663)\n",
      "Processing batch 132/354 (images 2097-2112/5663)\n",
      "Processing batch 133/354 (images 2113-2128/5663)\n",
      "Processing batch 134/354 (images 2129-2144/5663)\n",
      "Processing batch 135/354 (images 2145-2160/5663)\n",
      "Processing batch 136/354 (images 2161-2176/5663)\n",
      "Processing batch 137/354 (images 2177-2192/5663)\n",
      "Processing batch 138/354 (images 2193-2208/5663)\n",
      "Processing batch 139/354 (images 2209-2224/5663)\n",
      "Processing batch 140/354 (images 2225-2240/5663)\n",
      "Processing batch 141/354 (images 2241-2256/5663)\n",
      "Processing batch 142/354 (images 2257-2272/5663)\n",
      "Processing batch 143/354 (images 2273-2288/5663)\n",
      "Processing batch 144/354 (images 2289-2304/5663)\n",
      "Processing batch 145/354 (images 2305-2320/5663)\n",
      "Processing batch 146/354 (images 2321-2336/5663)\n",
      "Processing batch 147/354 (images 2337-2352/5663)\n",
      "Processing batch 148/354 (images 2353-2368/5663)\n",
      "Processing batch 149/354 (images 2369-2384/5663)\n",
      "Processing batch 150/354 (images 2385-2400/5663)\n",
      "Processing batch 151/354 (images 2401-2416/5663)\n",
      "Processing batch 152/354 (images 2417-2432/5663)\n",
      "Processing batch 153/354 (images 2433-2448/5663)\n",
      "Processing batch 154/354 (images 2449-2464/5663)\n",
      "Processing batch 155/354 (images 2465-2480/5663)\n",
      "Processing batch 156/354 (images 2481-2496/5663)\n",
      "Processing batch 157/354 (images 2497-2512/5663)\n",
      "Processing batch 158/354 (images 2513-2528/5663)\n",
      "Processing batch 159/354 (images 2529-2544/5663)\n",
      "Processing batch 160/354 (images 2545-2560/5663)\n",
      "Processing batch 161/354 (images 2561-2576/5663)\n",
      "Processing batch 162/354 (images 2577-2592/5663)\n",
      "Processing batch 163/354 (images 2593-2608/5663)\n",
      "Processing batch 164/354 (images 2609-2624/5663)\n",
      "Processing batch 165/354 (images 2625-2640/5663)\n",
      "Processing batch 166/354 (images 2641-2656/5663)\n",
      "Processing batch 167/354 (images 2657-2672/5663)\n",
      "Processing batch 168/354 (images 2673-2688/5663)\n",
      "Processing batch 169/354 (images 2689-2704/5663)\n",
      "Processing batch 170/354 (images 2705-2720/5663)\n",
      "Processing batch 171/354 (images 2721-2736/5663)\n",
      "Processing batch 172/354 (images 2737-2752/5663)\n",
      "Processing batch 173/354 (images 2753-2768/5663)\n",
      "Processing batch 174/354 (images 2769-2784/5663)\n",
      "Processing batch 175/354 (images 2785-2800/5663)\n",
      "Processing batch 176/354 (images 2801-2816/5663)\n",
      "Processing batch 177/354 (images 2817-2832/5663)\n",
      "Processing batch 178/354 (images 2833-2848/5663)\n",
      "Processing batch 179/354 (images 2849-2864/5663)\n",
      "Processing batch 180/354 (images 2865-2880/5663)\n",
      "Processing batch 181/354 (images 2881-2896/5663)\n",
      "Processing batch 182/354 (images 2897-2912/5663)\n",
      "Processing batch 183/354 (images 2913-2928/5663)\n",
      "Processing batch 184/354 (images 2929-2944/5663)\n",
      "Processing batch 185/354 (images 2945-2960/5663)\n",
      "Processing batch 186/354 (images 2961-2976/5663)\n",
      "Processing batch 187/354 (images 2977-2992/5663)\n",
      "Processing batch 188/354 (images 2993-3008/5663)\n",
      "Processing batch 189/354 (images 3009-3024/5663)\n",
      "Processing batch 190/354 (images 3025-3040/5663)\n",
      "Processing batch 191/354 (images 3041-3056/5663)\n",
      "Processing batch 192/354 (images 3057-3072/5663)\n",
      "Processing batch 193/354 (images 3073-3088/5663)\n",
      "Processing batch 194/354 (images 3089-3104/5663)\n",
      "Processing batch 195/354 (images 3105-3120/5663)\n",
      "Processing batch 196/354 (images 3121-3136/5663)\n",
      "Processing batch 197/354 (images 3137-3152/5663)\n",
      "Processing batch 198/354 (images 3153-3168/5663)\n",
      "Processing batch 199/354 (images 3169-3184/5663)\n",
      "Processing batch 200/354 (images 3185-3200/5663)\n",
      "Processing batch 201/354 (images 3201-3216/5663)\n",
      "Processing batch 202/354 (images 3217-3232/5663)\n",
      "Processing batch 203/354 (images 3233-3248/5663)\n",
      "Processing batch 204/354 (images 3249-3264/5663)\n",
      "Processing batch 205/354 (images 3265-3280/5663)\n",
      "Processing batch 206/354 (images 3281-3296/5663)\n",
      "Processing batch 207/354 (images 3297-3312/5663)\n",
      "Processing batch 208/354 (images 3313-3328/5663)\n",
      "Processing batch 209/354 (images 3329-3344/5663)\n",
      "Processing batch 210/354 (images 3345-3360/5663)\n",
      "Processing batch 211/354 (images 3361-3376/5663)\n",
      "Processing batch 212/354 (images 3377-3392/5663)\n",
      "Processing batch 213/354 (images 3393-3408/5663)\n",
      "Processing batch 214/354 (images 3409-3424/5663)\n",
      "Processing batch 215/354 (images 3425-3440/5663)\n",
      "Processing batch 216/354 (images 3441-3456/5663)\n",
      "Processing batch 217/354 (images 3457-3472/5663)\n",
      "Processing batch 218/354 (images 3473-3488/5663)\n",
      "Processing batch 219/354 (images 3489-3504/5663)\n",
      "Processing batch 220/354 (images 3505-3520/5663)\n",
      "Processing batch 221/354 (images 3521-3536/5663)\n",
      "Processing batch 222/354 (images 3537-3552/5663)\n",
      "Processing batch 223/354 (images 3553-3568/5663)\n",
      "Processing batch 224/354 (images 3569-3584/5663)\n",
      "Processing batch 225/354 (images 3585-3600/5663)\n",
      "Processing batch 226/354 (images 3601-3616/5663)\n",
      "Processing batch 227/354 (images 3617-3632/5663)\n",
      "Processing batch 228/354 (images 3633-3648/5663)\n",
      "Processing batch 229/354 (images 3649-3664/5663)\n",
      "Processing batch 230/354 (images 3665-3680/5663)\n",
      "Processing batch 231/354 (images 3681-3696/5663)\n",
      "Processing batch 232/354 (images 3697-3712/5663)\n",
      "Processing batch 233/354 (images 3713-3728/5663)\n",
      "Processing batch 234/354 (images 3729-3744/5663)\n",
      "Processing batch 235/354 (images 3745-3760/5663)\n",
      "Processing batch 236/354 (images 3761-3776/5663)\n",
      "Processing batch 237/354 (images 3777-3792/5663)\n",
      "Processing batch 238/354 (images 3793-3808/5663)\n",
      "Processing batch 239/354 (images 3809-3824/5663)\n",
      "Processing batch 240/354 (images 3825-3840/5663)\n",
      "Processing batch 241/354 (images 3841-3856/5663)\n",
      "Processing batch 242/354 (images 3857-3872/5663)\n",
      "Processing batch 243/354 (images 3873-3888/5663)\n",
      "Processing batch 244/354 (images 3889-3904/5663)\n",
      "Processing batch 245/354 (images 3905-3920/5663)\n",
      "Processing batch 246/354 (images 3921-3936/5663)\n",
      "Processing batch 247/354 (images 3937-3952/5663)\n",
      "Processing batch 248/354 (images 3953-3968/5663)\n",
      "Processing batch 249/354 (images 3969-3984/5663)\n",
      "Processing batch 250/354 (images 3985-4000/5663)\n",
      "Processing batch 251/354 (images 4001-4016/5663)\n",
      "Processing batch 252/354 (images 4017-4032/5663)\n",
      "Processing batch 253/354 (images 4033-4048/5663)\n",
      "Processing batch 254/354 (images 4049-4064/5663)\n",
      "Processing batch 255/354 (images 4065-4080/5663)\n",
      "Processing batch 256/354 (images 4081-4096/5663)\n",
      "Processing batch 257/354 (images 4097-4112/5663)\n",
      "Processing batch 258/354 (images 4113-4128/5663)\n",
      "Processing batch 259/354 (images 4129-4144/5663)\n",
      "Processing batch 260/354 (images 4145-4160/5663)\n",
      "Processing batch 261/354 (images 4161-4176/5663)\n",
      "Processing batch 262/354 (images 4177-4192/5663)\n",
      "Processing batch 263/354 (images 4193-4208/5663)\n",
      "Processing batch 264/354 (images 4209-4224/5663)\n",
      "Processing batch 265/354 (images 4225-4240/5663)\n",
      "Processing batch 266/354 (images 4241-4256/5663)\n",
      "Processing batch 267/354 (images 4257-4272/5663)\n",
      "Processing batch 268/354 (images 4273-4288/5663)\n",
      "Processing batch 269/354 (images 4289-4304/5663)\n",
      "Processing batch 270/354 (images 4305-4320/5663)\n",
      "Processing batch 271/354 (images 4321-4336/5663)\n",
      "Processing batch 272/354 (images 4337-4352/5663)\n",
      "Processing batch 273/354 (images 4353-4368/5663)\n",
      "Processing batch 274/354 (images 4369-4384/5663)\n",
      "Processing batch 275/354 (images 4385-4400/5663)\n",
      "Processing batch 276/354 (images 4401-4416/5663)\n",
      "Processing batch 277/354 (images 4417-4432/5663)\n",
      "Processing batch 278/354 (images 4433-4448/5663)\n",
      "Processing batch 279/354 (images 4449-4464/5663)\n",
      "Processing batch 280/354 (images 4465-4480/5663)\n",
      "Processing batch 281/354 (images 4481-4496/5663)\n",
      "Processing batch 282/354 (images 4497-4512/5663)\n",
      "Processing batch 283/354 (images 4513-4528/5663)\n",
      "Processing batch 284/354 (images 4529-4544/5663)\n",
      "Processing batch 285/354 (images 4545-4560/5663)\n",
      "Processing batch 286/354 (images 4561-4576/5663)\n",
      "Processing batch 287/354 (images 4577-4592/5663)\n",
      "Processing batch 288/354 (images 4593-4608/5663)\n",
      "Processing batch 289/354 (images 4609-4624/5663)\n",
      "Processing batch 290/354 (images 4625-4640/5663)\n",
      "Processing batch 291/354 (images 4641-4656/5663)\n",
      "Processing batch 292/354 (images 4657-4672/5663)\n",
      "Processing batch 293/354 (images 4673-4688/5663)\n",
      "Processing batch 294/354 (images 4689-4704/5663)\n",
      "Processing batch 295/354 (images 4705-4720/5663)\n",
      "Processing batch 296/354 (images 4721-4736/5663)\n",
      "Processing batch 297/354 (images 4737-4752/5663)\n",
      "Processing batch 298/354 (images 4753-4768/5663)\n",
      "Processing batch 299/354 (images 4769-4784/5663)\n",
      "Processing batch 300/354 (images 4785-4800/5663)\n",
      "Processing batch 301/354 (images 4801-4816/5663)\n",
      "Processing batch 302/354 (images 4817-4832/5663)\n",
      "Processing batch 303/354 (images 4833-4848/5663)\n",
      "Processing batch 304/354 (images 4849-4864/5663)\n",
      "Processing batch 305/354 (images 4865-4880/5663)\n",
      "Processing batch 306/354 (images 4881-4896/5663)\n",
      "Processing batch 307/354 (images 4897-4912/5663)\n",
      "Processing batch 308/354 (images 4913-4928/5663)\n",
      "Processing batch 309/354 (images 4929-4944/5663)\n",
      "Processing batch 310/354 (images 4945-4960/5663)\n",
      "Processing batch 311/354 (images 4961-4976/5663)\n",
      "Processing batch 312/354 (images 4977-4992/5663)\n",
      "Processing batch 313/354 (images 4993-5008/5663)\n",
      "Processing batch 314/354 (images 5009-5024/5663)\n",
      "Processing batch 315/354 (images 5025-5040/5663)\n",
      "Processing batch 316/354 (images 5041-5056/5663)\n",
      "Processing batch 317/354 (images 5057-5072/5663)\n",
      "Processing batch 318/354 (images 5073-5088/5663)\n",
      "Processing batch 319/354 (images 5089-5104/5663)\n",
      "Processing batch 320/354 (images 5105-5120/5663)\n",
      "Processing batch 321/354 (images 5121-5136/5663)\n",
      "Processing batch 322/354 (images 5137-5152/5663)\n",
      "Processing batch 323/354 (images 5153-5168/5663)\n",
      "Processing batch 324/354 (images 5169-5184/5663)\n",
      "Processing batch 325/354 (images 5185-5200/5663)\n",
      "Processing batch 326/354 (images 5201-5216/5663)\n",
      "Processing batch 327/354 (images 5217-5232/5663)\n",
      "Processing batch 328/354 (images 5233-5248/5663)\n",
      "Processing batch 329/354 (images 5249-5264/5663)\n",
      "Processing batch 330/354 (images 5265-5280/5663)\n",
      "Processing batch 331/354 (images 5281-5296/5663)\n",
      "Processing batch 332/354 (images 5297-5312/5663)\n",
      "Processing batch 333/354 (images 5313-5328/5663)\n",
      "Processing batch 334/354 (images 5329-5344/5663)\n",
      "Processing batch 335/354 (images 5345-5360/5663)\n",
      "Processing batch 336/354 (images 5361-5376/5663)\n",
      "Processing batch 337/354 (images 5377-5392/5663)\n",
      "Processing batch 338/354 (images 5393-5408/5663)\n",
      "Processing batch 339/354 (images 5409-5424/5663)\n",
      "Processing batch 340/354 (images 5425-5440/5663)\n",
      "Processing batch 341/354 (images 5441-5456/5663)\n",
      "Processing batch 342/354 (images 5457-5472/5663)\n",
      "Processing batch 343/354 (images 5473-5488/5663)\n",
      "Processing batch 344/354 (images 5489-5504/5663)\n",
      "Processing batch 345/354 (images 5505-5520/5663)\n",
      "Processing batch 346/354 (images 5521-5536/5663)\n",
      "Processing batch 347/354 (images 5537-5552/5663)\n",
      "Processing batch 348/354 (images 5553-5568/5663)\n",
      "Processing batch 349/354 (images 5569-5584/5663)\n",
      "Processing batch 350/354 (images 5585-5600/5663)\n",
      "Processing batch 351/354 (images 5601-5616/5663)\n",
      "Processing batch 352/354 (images 5617-5632/5663)\n",
      "Processing batch 353/354 (images 5633-5648/5663)\n",
      "Processing batch 354/354 (images 5649-5663/5663)\n",
      "Combining batches...\n",
      "Training model on 305146 examples with 511 features...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "memm = MEMMCaptionGenerator(\n",
    "    glove_path=\"glove.6B.100d.txt\",\n",
    "    cache_path=\"./feature_cache\",\n",
    "    batch_size=16,  # Process fewer images at once\n",
    "    use_pca=True   # Skip PCA to simplify\n",
    ")\n",
    "\n",
    "# Train\n",
    "memm.train(train_images, train_captions)\n",
    "\n",
    "# Save model\n",
    "memm.save_model(\"/Users/ruhwang/Desktop/AI/spring2025_courses/aipi540-dl/caption_generator/models/memm_caption_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
